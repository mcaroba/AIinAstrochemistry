{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import bayes_nanospace2025\n",
    "    print(\"Already installed\")\n",
    "except ImportError:\n",
    "    %pip install -q \"bayes_nanospace2025 @ git+https://github.com/Mads-PeterVC/nanospace2025.git\" # Install from GitHub. \n",
    "    print(\"Installed bayes_nanospace2025 from GitHub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from scipy.linalg import cho_solve, cho_factor\n",
    "from bayes_nanospace2025 import plot_gp_prediction, YourCodeHere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agenda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will explore Gaussian Processes; \n",
    "\n",
    "The notebook 4 main sections: \n",
    "\n",
    "1. The Gaussian process prior\n",
    "2. The Gaussian process Posterior\n",
    "3. Wrapping things into a versatile GP class.\n",
    "4. Using GP model and choosing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "There are small exercises dispersed throughout the notebook, they are indicated by a `YourCodeHere`-function that will raise an error if it has not been replaced by your code. \n",
    "As an example, if you encouter something like the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "c = YourCodeHere(\"Add a and b together\")\n",
    "assert c == 2, \"The sum of a and b should be 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should replace it with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = 1\n",
    "c = a + b\n",
    "assert c == 2, \"The sum of a and b should be 2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by seeing how to draw samples from a GP prior, where \n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(0, K(X, X))\n",
    "$$\n",
    "\n",
    "So we will need to decide on a domain for $X$ and implement a kernel function $K$.\n",
    "We will start using the radial basis function (RBF) kernel, \n",
    "\n",
    "$$\n",
    "K(x, x^*) = \\exp{\\left(-\\frac{|x-x^*|^2}{2\\lambda^2}\\right)}\n",
    "$$\n",
    "With $|x|$ being the euclidean distance of $x$. \n",
    "\n",
    "In the cell below, implement the RBF kernel \n",
    "\n",
    "<details>\n",
    "  <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint (Click arrow to expand) </span></strong></summary>\n",
    "  <p>With numpy array operations you can calculate K for all distances in one call. E.g. np.exp(dists) applies exponential function to all elements in dists.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RBF(x1, x2, length_scale=1.0):\n",
    "    \"\"\"\n",
    "    Radial Basis Function (RBF) kernel.\n",
    "    \"\"\"\n",
    "    dists = cdist(\n",
    "        x1, x2, \"sqeuclidean\"\n",
    "    )  # This computes the squared Euclidean distance between all pairs of points in x1 and x2.\n",
    "    K = YourCodeHere(\"Implement the RBF kernel.\")  # Your code here.\n",
    "    return K\n",
    "\n",
    "X = np.linspace(0, 1, 10).reshape(-1, 1)  # 100 points from 0 to 10\n",
    "assert RBF(X, X).shape == (10, 10), \"The kernel matrix should be of shape (10, 10)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a kernel we can calculate the covariancee between some set of points $X$. \n",
    "\n",
    "In the cell below call the RBF function and it will plot the covariance matrix. \n",
    "\n",
    "Try it for different values of the `length_scale`-parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 1, 100).reshape(-1, 1)  # Create a grid of points\n",
    "K = YourCodeHere(\"Compute the covariance matrix using the RBF kernel.\")  # Your code here.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "im = ax.matshow(K, origin=\"upper\", extent=[0, 1, 1, 0], cmap=\"Purples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to sample some functions to do so we will\n",
    "\n",
    "- Define a set of points $X$. \n",
    "- Calculate the covariance between those points using our kernel function. \n",
    "- Decide on a prior mean, which can just be $\\mu(X) = 0$.\n",
    "- Sample from a multivariate normal distribution with the mean and covariance we've defined.\n",
    "\n",
    "The last step is handled by the `np.random.multivariate_normal` function which can be called like so: `np.random.multivariate_normal(mean, covariance, num_samples)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define set of points\n",
    "X = YourCodeHere(\"Create a set of points of shape (n, 1) where n is the number of points.\")  # Your code here.\n",
    "\n",
    "# Calculate the covariance between those points using our kernel function\n",
    "K = YourCodeHere(\"Calculate the covariance matrix using the RBF kernel.\")  # Your code here.\n",
    "\n",
    "# Define a prior mean\n",
    "mean = np.zeros(X.shape[0])  # Mean function, zero everywhere\n",
    "\n",
    "# Sample from a multivariate normal distribution with the mean and covariance we've defined\n",
    "n_samples = 10\n",
    "sample = YourCodeHere(\"Sample from a multivariate normal distribution with the mean and covariance defined above.\")  # Your code here.\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "l1 = ax.plot(X, sample.T, color=\"gray\")\n",
    "ax.plot(X, mean, color=\"black\", lw=2, label=\"Mean\")\n",
    "ax.set_xlim(X.min(), X.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it - these are samples from the Gaussian Process prior! Now let's try changing the kernel hyperparameters and see how that influences the sampled functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scales = [0.01, 0.1, 0.25, 0.5, 1.0]  # Different length scales to explore\n",
    "X = np.linspace(0, 1, 100).reshape(-1, 1)  # Create a grid of points\n",
    "mean = np.zeros(X.shape[0])  # Mean function, zero everywhere\n",
    "n_samples = 10  # Number of samples to draw\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(5 * 3, 3), sharey=True, layout=\"constrained\")\n",
    "for ax, length_scale in zip(axes, length_scales):\n",
    "    samples = np.random.multivariate_normal(mean, RBF(X, X, length_scale), n_samples)\n",
    "    ax.plot(X, samples.T, color=\"gray\")\n",
    "    ax.set_title(f\"Length Scale = {length_scale}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"><b>Takeaway:</b> The types of functions we get are clearly controlled by the kernel and it's hyperparameters. </div>\n",
    "\n",
    "There are many choices of kernels, such as dot product kernel\n",
    "$$\n",
    "K(x, x^*) = (x \\cdot x^*)\n",
    "$$\n",
    "\n",
    "Periodic kernel\n",
    "$$\n",
    "K(x, x^*) = \\exp{\\left( -\\frac{2}{\\lambda^2}\\sin^2\\left( \\frac{\\pi |x-x^*|}{p} \\right) \\right)}\n",
    "$$\n",
    "\n",
    "In the cell below finish implementing the periodic kernel.\n",
    "\n",
    "<details>\n",
    "    <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint - Dot product </span></strong></summary>\n",
    "    <p> This is easist if you compute the outer-product between x1 and x2 - e.g. with np.outer </p>\n",
    "  </details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "    <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint - Periodic </span></strong></summary>\n",
    "    <p> Again using array operations this can all be vectorized, so using np.sin(x) computes the sine of all values in x. </p>\n",
    "  </details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot(x1, x2):\n",
    "    \"\"\"\n",
    "    Dot product between two vectors.\n",
    "    \"\"\"\n",
    "    YourCodeHere(\"Compute the dot product between two vectors, make sure it returns a matrix of shape (n, m) where n is the number of points in x1 and m is the number of points in x2.\")  # Your code here.\n",
    "\n",
    "\n",
    "def periodic(x1, x2, length_scale=1.0, period=1.0):\n",
    "    \"\"\"\n",
    "    Periodic kernel.\n",
    "    \"\"\"\n",
    "    dists = cdist(x1, x2, \"euclidean\")  # This computes the Euclidean distance between all pairs of points in x1 and x2.\n",
    "    K = YourCodeHere(\"Implement the periodic kernel.\")  # Your code here.\n",
    "    return K\n",
    "\n",
    "X1 = np.linspace(0, 1, 10).reshape(-1, 1)\n",
    "X2 = np.linspace(0, 1, 15).reshape(-1, 1)\n",
    "\n",
    "assert dot(X1, X2).shape == (10, 15), \"The dot product should return a matrix of shape (n, m) where n is the number of points in X1 and m is the number of points in X2.\"\n",
    "assert periodic(X1, X2, length_scale=0.1, period=0.5).shape == (10, 15), \"The periodic kernel should return a matrix of shape (n, m) where n is the number of points in X1 and m is the number of points in X2.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A nice property of kernel functions is that they can be used as building blocks of more complex kernels just by adding and multiplying them together - kernels are **composable**.\n",
    "\n",
    "For example, we can create a polynomial kernel as the product of two dot product kernels. \n",
    "\n",
    "<details>\n",
    "    <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint - Lambda functions </span></strong></summary>\n",
    "    <p> See how the kernels in the kernels list are defined using lambda functions, you can do the same \n",
    "    and just multiply or add kernels together. </p>\n",
    "  </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\n",
    "    lambda x1, x2: RBF(x1, x2, length_scale=0.1),\n",
    "    lambda x1, x2: dot(x1, x2),\n",
    "    lambda x1, x2: periodic(x1, x2, length_scale=0.5, period=np.pi / 8),\n",
    "]\n",
    "\n",
    "# Try to make a quadratic kernel as the product of two dot product kernels\n",
    "quadratic_kernel = YourCodeHere(\"Implement the quadratic kernel as the product of two dot product kernels in a lambda function .. lambda x1, x2: <your code here>\")  # Your code here.\n",
    "kernels.append(quadratic_kernel)\n",
    "\n",
    "# Can also make a more complex kernel by adding two kernels together\n",
    "complex_kernel = lambda x1, x2: RBF(x1, x2, length_scale=0.1) * periodic(x1, x2, length_scale=0.5, period=np.pi / 8) + dot(x1, x2) \n",
    "kernels.append(complex_kernel)\n",
    "\n",
    "\n",
    "length_scales = [0.01, 0.1, 0.25, 0.5, 1.0]  # Different length scales to explore\n",
    "X = np.linspace(0, 1, 100).reshape(-1, 1)  # Create a grid of points\n",
    "mean = np.zeros(X.shape[0])  # Mean function, zero everywhere\n",
    "n_samples = 3  # Number of samples to draw\n",
    "\n",
    "ncols = len(kernels)\n",
    "\n",
    "labels = ['RBF', 'Dot Product', 'Periodic', 'Quadratic', 'Complex']\n",
    "\n",
    "fig, axes = plt.subplots(1, ncols, figsize=(ncols * 3, 3), sharey=True, layout=\"constrained\")\n",
    "for ax, kernel, label in zip(axes, kernels, labels):\n",
    "    covariance = kernel(X, X)\n",
    "    samples = np.random.multivariate_normal(mean, covariance, n_samples)\n",
    "    ax.plot(X, samples.T)\n",
    "    ax.set_title(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Process Prior for functions of multiple variables.\n",
    "\n",
    "So far we've looked only at functions of a single variable, but there's really nothing special about functions of multiple variables - we just need our kernel to describe the covariance \n",
    "between sets of multiple variables. \n",
    "\n",
    "We've had that our \"index set\" $X$ is something like this \n",
    "\n",
    "$$\n",
    "X = [x_1, x_2, x_3, ..., x_N]\n",
    "$$\n",
    "\n",
    "For more dimensions we just need to redefine our set as \n",
    "\n",
    "$$\n",
    "X = [(x_1, y_1), (x_2, y_2), (x_3, y_3), ..., (x_N, y_N)]\n",
    "$$\n",
    "\n",
    "The cell below defines a 2D index set and calculates the RBF kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a set of points in 2D\n",
    "x = np.linspace(0, 1, 50)\n",
    "y = np.linspace(0, 1, 50)\n",
    "x_mesh, y_mesh = np.meshgrid(x, y)\n",
    "X = np.vstack([x_mesh.ravel(), y_mesh.ravel()]).T\n",
    "\n",
    "print(f\"Shape of X: {X.shape}\")  # Should be (2500, 2) for a 50x50 grid\n",
    "\n",
    "# Calculate the covariance between those points using our kernel function\n",
    "K = YourCodeHere(\"Calculate the covariance K(X, X) using one of our kernels - this is no different from the 1D case!\")  # Your code here.\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "im = ax.matshow(K, origin=\"upper\", cmap=\"Purples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.zeros(X.shape[0])  # Mean function, zero everywhere\n",
    "n_samples = 3  # Number of samples to draw\n",
    "samples = YourCodeHere(\"Sample from a multivariate normal distribution with the mean and covariance defined above.\")  # Your code here.\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(3 * 3, 3), sharey=True, layout=\"constrained\")\n",
    "\n",
    "for ax, sample in zip(axes, samples):\n",
    "    ax.contour(x_mesh, y_mesh, sample.reshape(x_mesh.shape), colors=\"black\")\n",
    "    ax.contourf(x_mesh, y_mesh, sample.reshape(x_mesh.shape), cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Takeaway:</b> GP's generalize naturally to multidimensional functions with everything handled by the kernel. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Process Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to use the GP for regression we would like to not just draw random functions, so we condition the GP on the observations. \n",
    "Our observations are pairs of coordinates and function values $(X, \\mathbf{y})$.\n",
    "\n",
    "This leads to following equations for the posterior mean and posterior covariance\n",
    "$$\n",
    "\\mu(X^*) = K(X^*, X)[K(X, X)+\\sigma_n^2I]^{-1}\\mathbf{y}\n",
    "$$ \n",
    "\n",
    "$$\n",
    "\\Sigma(X^*, X^*) = K(X^*, X^*) - K(X^*, X)[K(X, X) + \\sigma^2_n I]^{-1}K(X, X^*)\n",
    "$$\n",
    "\n",
    "Where $X^*$ are where we want to query the GP and $\\sigma_n^2$ is our assumed noise on the observations.\n",
    "\n",
    "Both the posterior mean and covariance involve inverting the covariance matrix of the observations\n",
    "\n",
    "$$\n",
    "[K(X, X) + \\sigma^2_n I]^{-1}\n",
    "$$\n",
    "\n",
    "Some care needs to be taken to do this in a stable and accurate way. \n",
    "\n",
    "---\n",
    "\n",
    "Typically the most numerically stable and efficient way of doing so for the types of matrices encountered with GP's is Cholesky decomposition. \n",
    "With Cholesky decomposition a matrix $A$ is decomposed like so \n",
    "\n",
    "$$\n",
    "A = LL^*\n",
    "$$\n",
    "\n",
    "The matrix $L$ can then be used to efficiently solve for the inverse of $A$ and for computing the determinant of $A$. The core routines of this \n",
    "are implemented in SciPy\n",
    "\n",
    "- `scipy.linalg.cho_factor`: Performs the factorization to get the $L$-matrix.\n",
    "- `cho_solve`: Solves the linear system of $A x = B$ given the $L$-matrix of $A$. We set $B = I$ to find the inverse.\n",
    "\n",
    "---\n",
    "\n",
    "In the cell below we will start by computing the posterior mean.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some observations\n",
    "X = np.array([0.3, 0.5, 0.7]).reshape(-1, 1)\n",
    "y = np.array([-0.5, 0.5, 0]).reshape(-1, 1)\n",
    "\n",
    "# Define the noise level\n",
    "noise = 0.001\n",
    "length_scale = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the covariance matrix for the observations\n",
    "K_obs = RBF(X, X, length_scale=length_scale) # Compute the kernel matrix for the observations.\n",
    "K_obs += noise * np.eye(len(X))  # Add noise to the diagonal to include observation noise.\n",
    "\n",
    "# Use Cholesky decomposition to compute the inverse of the covariance matrix\n",
    "L, _ = cho_factor(K_obs, True)  # Cholesky factorization\n",
    "K_obs_inv = cho_solve((L, True), np.eye(len(X)))  # Inverse of the covariance matrix\n",
    "\n",
    "# Evaluate the posterior mean\n",
    "X_query = np.linspace(0, 1, 100).reshape(-1, 1) # Points where we want to make predictions\n",
    "K_query = RBF(X_query, X, length_scale=length_scale) # Covariance of query points with observations, make sure to use the same length scale as in K_obs.\n",
    "\n",
    "print(f\"{K_query.shape = } - Should be (100, 3)\")\n",
    "print(f\"{K_obs_inv.shape = } - Should be (3, 3)\")\n",
    "print(f\"{y.shape = }         - Should be (3, 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to compute the posterior mean as the dot product of the computed matrices\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mu(X^*) &= K_{query} \\cdot K_{obs}^{-1} \\cdot \\mathbf{y} \\\\\n",
    "&= K_{\\mathrm{query}} \\cdot K_{\\mathrm{obs\\_inv}} \\cdot \\mathbf{y}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$ \n",
    "\n",
    "<details>\n",
    "    <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint: Dot products </span></strong></summary>\n",
    "    <p> In NumPy the dot product can be calculated using either np.dot(a, b) or a @ b. </p>\n",
    "  </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_posterior = YourCodeHere(\"Calculate the posterior mean using the covariance between query points and observations, the inverse of the covariance matrix of observations, and the observed values.\")  # Your code here.\n",
    "mu_posterior = mu_posterior.flatten()  # Flatten the mean to make it a 1D array for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "\n",
    "ax.plot(X_query, mu_posterior, color=\"blue\", label=\"Posterior Mean\")\n",
    "ax.scatter(X, y, color=\"red\", label=\"Observations\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've done it right you should see that the posterior mean closely matches the observations.\n",
    "\n",
    "If that's the case, we can move onto the posterior covariance, we will also calculate posterior standard deviation \n",
    "\n",
    "$$\n",
    "\\sigma(X^*_i) = \\Sigma(X^*_i, X^*_i)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathrm{std}(X^*_i) = \\sqrt{\\sigma(X^*_i)}\n",
    "$$\n",
    "\n",
    "Or in other words the standard deviations are the square root of the diagonal elements. Again, the posterior covariance is given by\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\Sigma(X^*, X^*) &= K(X^*, X^*) - K(X^*, X)[K(X, X) + \\sigma^2_n I]^{-1}K(X, X^*) \\\\\n",
    "&= K_\\mathrm{qq} - K_\\mathrm{query} \\cdot K_{\\mathrm{obs\\_inv}} \\cdot K_\\mathrm{query}^T\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "<details>\n",
    "    <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint </span></strong></summary>\n",
    "    <p> This is again a dot product between the computed matrices, so use np.dot or @ to compute the second term. </p>\n",
    "  </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_qq = RBF(X_query, X_query, length_scale=length_scale) # The covariance matrix for the query points, again same length scale as in K_obs.\n",
    "cov_posterior = YourCodeHere(\"Compute the covariance of the posterior distribution.\")  # Your code here.\n",
    "\n",
    "var_posterior = np.diag(cov_posterior)  # Variance of the posterior distribution\n",
    "std_posterior = np.sqrt(var_posterior)  # Standard deviation of the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have everything lets just check the shapes and confirm that what we have is \n",
    "\n",
    "- The posterior mean $\\mu_\\mathrm{posterior}$ as an array with as many elements $N_\\mathrm{query}$ as were in $X_\\mathrm{query}$. \n",
    "- The posterior covariance as a matrix of shape $N_\\mathrm{query} \\times N_\\mathrm{query}$.\n",
    "- Posterior variance and standard deviation each with $N_\\mathrm{query}$ elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{mu_posterior.shape = } - Should be (100,)\")\n",
    "print(f\"{cov_posterior.shape = } - Should be (100, 100)\")\n",
    "print(f\"{var_posterior.shape = } - Should be (100,)\")\n",
    "print(f\"{std_posterior.shape = } - Should be (100,)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the posterior mean and covariance we can draw samples from the posterior distribution\n",
    "n_samples = 10  # Number of samples to draw\n",
    "samples = np.random.multivariate_normal(mu_posterior, cov_posterior, n_samples, check_valid=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "ax.scatter(X, y, color=\"red\", label=\"Observations\", zorder=5)\n",
    "ax.plot(X_query, mu_posterior, color=\"blue\", label=\"Posterior Mean\")\n",
    "ax.fill_between(\n",
    "    X_query.flatten(),\n",
    "    mu_posterior.flatten() + 2 * std_posterior.flatten(),\n",
    "    mu_posterior.flatten() - 2 * std_posterior.flatten(),\n",
    "    color=\"blue\",\n",
    "    alpha=0.2,\n",
    "    label=\"Posterior +/- 2 std\",\n",
    ")\n",
    "\n",
    "l = ax.plot(X_query, samples.T, color=\"gray\", alpha=0.5)\n",
    "ax.legend()\n",
    "ax.set_xlim(X_query.min(), X_query.max());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Takeaway:</b> You've now implemented the predictive equations of a GP! </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log marginal likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we want to be able to calculate is the **log marginal likelihood** a measure of how well the Gaussian Process with given hyperparameters explains the observed data, balancing data fit and model complexity.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\mathrm{LML} = \\log p(y | X ) =& -\\frac{1}{2} \\mathbf{y}^T(K(X, X) + \\sigma^2_n I)^{-1}\\mathbf{y} \\\\\n",
    "& -\\frac{1}{2} \\log |K(X, X) + \\sigma^2_n I| \\\\\n",
    "& - \\frac{n}{2} \\log 2\\pi\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $|...|$ in the second term is the determinant of the matrix. The three terms can be interpreted as \n",
    "\n",
    "1. Data fit - how well do the function in the distribution fit the data?\n",
    "2. Model complexity - how complex is the model? \n",
    "3. A normalization constant. \n",
    "\n",
    "The first and the second term thus strike some balance that helps avoid overfitting. \n",
    "\n",
    "The first term includes the matrix inversion that've already seen how to do with Cholesky decomposition. \n",
    "The second term requires calculating the determinant of the same matrix, to ensure numerical stability a little bit of thought \n",
    "is required here. \n",
    "\n",
    "----\n",
    "\n",
    "However, it turns out that the determinant can be calculated from the Cholesky factorization $L$. \n",
    "Again for a matrix $A$ the Cholesky decompositions is defined through \n",
    "$$\n",
    "A = LL^*\n",
    "$$\n",
    "The determinant of $A$ can be stated as \n",
    "$$\n",
    "|A| = \\prod_i^n L_{ii}^2\n",
    "$$\n",
    "And therefore the log-determinant is \n",
    "$$\n",
    "\\log |A| = 2 \\sum_i^n \\log L_{ii}\n",
    "$$\n",
    "We thus get the log-determinant as the sum of the log of the diagonal elements of $L$ - which offers better numerical accuracy and stability compared to \n",
    "other methods in my experience. \n",
    "\n",
    "----\n",
    "\n",
    "The LML is typically used to choose hyperparameters, so we will write a function that computes the LML given some data and the GP hyperparameters.\n",
    "\n",
    "<details>\n",
    "    <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint 1st LML term: </span></strong></summary>\n",
    "    <p> The first term is a dot, one way of calculating it is: -0.5 * y.T @ K_inv @ y </p>\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "    <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint 2nd LML term: </span></strong></summary>\n",
    "    <p> The second term is the sum of the logarithm of the diagional of L: -np.sum(np.log(np.diag(L))) </p>\n",
    "</details>\n",
    "\n",
    "<br>\n",
    "\n",
    "<details>\n",
    "    <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint 3rd LML term: </span></strong></summary>\n",
    "    <p> This is a constant given by -n / 2 * np.log(2 * np.pi) </p>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lml(X, y, length_scale=1.0, noise=0.001, amplitude=1.0):\n",
    "    \"\"\"\n",
    "    Log Marginal Likelihood (LML) for Gaussian Process Regression.\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "\n",
    "    K = amplitude * RBF(X, X, length_scale)  # Covariance matrix\n",
    "    K += noise * np.eye(n)  # Add noise to the diagonal\n",
    "\n",
    "    L, _ = cho_factor(K, True)  # Cholesky factorization\n",
    "    K_inv = cho_solve((L, True), np.eye(n))  # Solve for K_inv\n",
    "\n",
    "    lml_value = 0\n",
    "    lml_value + = YourCodeHere(\"Compute the first term of the log marginal likelihood.\")  # Your code here.\n",
    "    lml_value + = YourCodeHere(\"Compute the second term of the log marginal likelihood.\")  # Your code here.\n",
    "    lml_value + = YourCodeHere(\"Compute the third term of the log marginal likelihood.\")  # Your code here.\n",
    "\n",
    "    return lml_value.flatten()[0]  # Return as a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([0.3, 0.5, 0.7]).reshape(-1, 1)\n",
    "y = np.array([-0.5, 0.5, 0]).reshape(-1, 1)\n",
    "\n",
    "lml(X, y, length_scale=0.1, noise=0.001)  # Should give -3.0315"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_scales = [0.1, 0.25, 0.5, 1.0]\n",
    "noise = 0.001\n",
    "X = np.array([0.3, 0.5, 0.7]).reshape(-1, 1)\n",
    "y = np.array([-0.5, 0.5, 0]).reshape(-1, 1)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(4 * 4, 4), sharey=True, layout=\"constrained\")\n",
    "\n",
    "for ax, ls in zip(axes, length_scales):\n",
    "    lml_value = lml(X, y, length_scale=ls, noise=noise)\n",
    "\n",
    "    K_obs = RBF(X, X, length_scale=ls) # Compute the kernel matrix for the observations.\n",
    "    K_obs += noise * np.eye(len(X))  # Add noise to the diagonal to include observation noise.\n",
    "\n",
    "    # Use Cholesky decomposition to compute the inverse of the covariance matrix\n",
    "    L, _ = cho_factor(K_obs, True)  # Cholesky factorization\n",
    "    K_obs_inv = cho_solve((L, True), np.eye(len(X)))  # Inverse of the covariance matrix\n",
    "\n",
    "    # Evaluate the posterior mean\n",
    "    X_query = np.linspace(0, 1, 100).reshape(-1, 1) # Points where we want to make predictions\n",
    "    K_query = RBF(X_query, X, length_scale=ls) # Covariance of query points with observations, make sure to use the same length scale as in K_obs.\n",
    "\n",
    "    mu_posterior = K_query @ K_obs_inv @ y \n",
    "    mu_posterior = mu_posterior.flatten()\n",
    "\n",
    "    K_qq = RBF(X_query, X_query, length_scale=ls) # The covariance matrix for the query points, again same length scale as in K_obs.\n",
    "    cov_posterior = K_qq - K_query @ K_obs_inv @ K_query.T # Posterior covariance\n",
    "\n",
    "    var_posterior = np.diag(cov_posterior)  # Variance of the posterior distribution\n",
    "    std_posterior = np.sqrt(var_posterior)  # Standard deviation of the posterior\n",
    "\n",
    "    # With the posterior mean and covariance we can draw samples from the posterior distribution\n",
    "    n_samples = 10  # Number of samples to draw\n",
    "    samples = np.random.multivariate_normal(mu_posterior, cov_posterior, n_samples, check_valid=\"ignore\")\n",
    "\n",
    "    ax.scatter(X, y, color=\"red\", label=\"Observations\", zorder=5)\n",
    "    ax.plot(X_query, mu_posterior, color=\"blue\", label=\"Posterior Mean\")\n",
    "    ax.fill_between(\n",
    "        X_query.flatten(),\n",
    "        mu_posterior.flatten() + 2 * std_posterior.flatten(),\n",
    "        mu_posterior.flatten() - 2 * std_posterior.flatten(),\n",
    "        color=\"blue\",\n",
    "        alpha=0.2,\n",
    "        label=\"Posterior +/- 2 std\",\n",
    "    )\n",
    "\n",
    "    l = ax.plot(X_query, samples.T, color=\"gray\", alpha=0.5)\n",
    "    ax.legend()\n",
    "    ax.set_title(f\"Length Scale = {ls}, LML = {lml_value:.4f}\")\n",
    "    ax.set_xlim(X_query.min(), X_query.max());\n",
    "# We can now define a Gaussian Process model that uses the RBF kernel and allows us to condition on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Takeaway:</b> We can use the LML to judge if we are describing the data \n",
    "well - we want a large LML as that indicates the data is likely under the model. Notice that in the above figure \n",
    "we get an LML value of about -3 with a length-scale of 0.1, and it decreases as the length-scale increases - until \n",
    "at a length-scale of 1.0 our GP posterior doesn't even get close to the data-points. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Creating a GPR Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Note:</b> You can skip this part and use my implementation instead. But it is good practice to refactor code after having explored it in a less structured way. </div>\n",
    "\n",
    "So far we've seen all the elements required to construct and utilize a Gaussian process\n",
    "- The kernel.\n",
    "- The prior mean and covariance.\n",
    "- The posterior mean and covariance.\n",
    "- Sampling from the prior or the posterior.\n",
    "- Calculating the log marginal likelihood.\n",
    "\n",
    "However, it is not particularly structured, reusable or versatile. It's typically quite useful to wrap \n",
    "all these elements in a \"Model\"-object, similar to a `torch.Module`, such that when using it we don't need to think \n",
    "about carrying the right matrices around. \n",
    "\n",
    "This is really a software design thing and there are a number of ways to do so each with benefits for certain use cases.\n",
    "However, it is typically much easier to understand, use and extend an algorithm if it is written in a concise way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel class\n",
    "\n",
    "I've implemented a simple `Kernel`-base class that's imported below. \n",
    "\n",
    "It really only handles two things; \n",
    "- Allows composition of kernels through addition and multiplication.\n",
    "- Defines a consistent API. \n",
    "\n",
    "To implement a kernel with this, two methods have to be defined\n",
    "\n",
    "- `__init__`: Set the kernel hyperparameters. \n",
    "- `__call__`: Compute the kernel given two arrays.\n",
    "\n",
    "In the cell below implement the RBF kernel by completing the `__call__` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_nanospace2025.tutorial.kernels import Kernel\n",
    "\n",
    "\n",
    "class RadialBasis(Kernel):\n",
    "    \"\"\"\n",
    "    Radial Basis Function (RBF) kernel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, length_scale=1.0):\n",
    "        super().__init__()\n",
    "        self.length_scale = length_scale\n",
    "\n",
    "    def __call__(self, x1, x2=None):\n",
    "        if x2 is None:\n",
    "            x2 = x1\n",
    "        dists = cdist(x1, x2, \"sqeuclidean\")\n",
    "        YourCodeHere(\"Implement the RBF kernel - which you've already done once.\")  # Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With composable kernels we can include noise as a kernel.\n",
    "\n",
    "There are some implementation subtleties with this, we need the implementation to \n",
    "behave like so: \n",
    "\n",
    "- $K(X, X)$: Noise added to the diagonal.\n",
    "- $K(X, Y)$: No noise added. \n",
    "\n",
    "We accomplish this by defining that $K(X) = K(X, X)$ - so noise is only added if only one \n",
    "input set is provided. \n",
    "\n",
    "<details>\n",
    "    <summary><strong><span style=\"color: lightgreen; font-size: 18px;\">ðŸ’¡ Hint - Identity matrix </span></strong></summary>\n",
    "    <p> To create an identity matrix use np.eye(n) to get an n by n matrix. This can then be multiplied by the value we want on the diagonal. </p>\n",
    "  </details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise(Kernel):\n",
    "    \"\"\"\n",
    "    Noise kernel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, noise_level=1e-3):\n",
    "        super().__init__()\n",
    "        self.noise_level = noise_level\n",
    "\n",
    "    def __call__(self, x1, x2=None):\n",
    "        if x2 is not None:\n",
    "            return np.zeros((x1.shape[0], x2.shape[0]))\n",
    "        else:\n",
    "            K = YourCodeHere(\"Return a diagonal matrix with noise_level on the diagonal for the noise kernel.\")  # Your code here.\n",
    "            return K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that this behaves as expected;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = RadialBasis(length_scale=0.1) + Noise(noise_level=0.001)\n",
    "\n",
    "X = np.linspace(0, 1, 10).reshape(-1, 1)  # Create a grid of points\n",
    "\n",
    "Kx = kernel(X)  # Covariance matrix with itself -> Noise added.\n",
    "Kxy = kernel(X, X)  # Covariance matrix for two inputs -> No noise added.\n",
    "\n",
    "print(f\"{Kx.max() = } - Should be 1.001\")\n",
    "print(f\"{Kxy.max() = } - Should be 1.000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will add a `Constant` kernel that just acts as an amplitude.\n",
    "\n",
    "$$\n",
    "K(X, X) = \\sigma_f\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Constant(Kernel):\n",
    "    \"\"\"\n",
    "    Constant kernel.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, amplitude=1.0):\n",
    "        super().__init__()\n",
    "        self.amplitude = amplitude\n",
    "\n",
    "    def __call__(self, x1, x2=None):\n",
    "        return self.amplitude "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can do a little bit of sanity checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.linspace(0, 1, 10).reshape(-1, 1)  # Create a grid of points\n",
    "kernel = Constant(amplitude=2.0) * RadialBasis(length_scale=0.1) + Noise(noise_level=0.001)\n",
    "\n",
    "K = kernel(X, X)\n",
    "print(f\"{K.max() = } - Should be 2.0\")\n",
    "print(f\"{K.min() = } - Should be very small.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have enough kernels to continue, but if you want to implement e.g. the periodic or the dot product kernel as \n",
    "one the classes feel free to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GP Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement a `GaussianProcess`-class. \n",
    "\n",
    "To keep things neat I've defined two dataclasses \n",
    "\n",
    "- `PosteriorState`: To contain all the information needed to calculate properties of the posterior. \n",
    "- `PredictionResult`: To contain predicted properties, e.g. the mean, variance and covariance at the given query points.\n",
    "\n",
    "Additionally, this implementation a couple of minor differences compared to what we've seen before. \n",
    "Firstly, the prior mean can be set - this changes the equation for the posterior mean: \n",
    "\n",
    "$$\n",
    "\\mu(X^*) = K(X^*, X)[K(X, X)+\\sigma_n^2I]^{-1}(\\mathbf{y}-\\mu_\\mathrm{prior}(X)) + \\mu_\\mathrm{prior}(X^*)\n",
    "$$\n",
    "\n",
    "Which is equal to what we've used so far, if $\\mu_\\mathrm{prior} = 0$, this can be useful to e.g. set \n",
    "the prior mean to the mean of the observed data reducing the magnitude of what the GP has to fit. \n",
    "\n",
    "Secondly, we have now included data noise in the kernel, and therefore we do not need to explicitly add it \n",
    "when calculating the inverse\n",
    "\n",
    "$$\n",
    "[K(X, X)+\\sigma_n^2I]^{-1} \\rightarrow [K(X, X)+\\sigma_\\mathrm{jitter}I]^{-1}\n",
    "$$\n",
    "\n",
    "However, for numerical stability a very small diagonal noise term is still added - this is called a *jitter*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PosteriorState:\n",
    "    X_train: np.ndarray  # Training inputs\n",
    "    y_train: np.ndarray  # Training outputs\n",
    "    K_inv: np.ndarray  # Inverse of the covariance matrix for training inputs\n",
    "    alpha: np.ndarray  # Vector for computing the posterior mean\n",
    "    cholesky_factor: np.ndarray # Cholesky L-matrix for the covariance matrix.\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PredictionResult:\n",
    "    X_query: np.ndarray  # Query points where predictions are made\n",
    "    mean: np.ndarray  # Posterior mean at query points\n",
    "    variance: np.ndarray  # Posterior variance at query points\n",
    "    covariance: np.ndarray  # Posterior covariance matrix at query points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GaussianProcess:\n",
    "    def __init__(self, kernel: Kernel, prior_mean: Callable | float | None = None, jitter: float = 1e-9):\n",
    "        self.kernel = kernel\n",
    "        self.jitter = jitter\n",
    "        self.prior_mean = prior_mean if prior_mean is not None else 0\n",
    "        self.state = None  # This will hold the posterior state after conditioning on observed data\n",
    "\n",
    "    def condition(self, X_obs, y_obs) -> None:\n",
    "        \"\"\"\n",
    "        Condition the Gaussian Process on observed data.\n",
    "        \"\"\"\n",
    "        K_obs = self.kernel(X_obs)  # Covariance matrix for the observations\n",
    "        K_obs += self.jitter * np.eye(len(X_obs))  # Add jitter to the diagonal to ensure numerical stability\n",
    "\n",
    "        # Use Cholesky decomposition to compute the inverse of the covariance matrix\n",
    "        L, _ = cho_factor(K_obs, lower=True)  # Cholesky factorization\n",
    "        K_obs_inv = cho_solve((L, True), np.eye(len(X_obs)))  # Inverse of the covariance matrix\n",
    "\n",
    "        # If prior_mean is callable, evaluate it at X_obs; otherwise, use a constant prior mean\n",
    "        prior_mean = self.prior_mean(X_obs) if callable(self.prior_mean) else np.full((len(X_obs), 1), self.prior_mean)\n",
    "        y = y_obs - prior_mean  # Adjust observed outputs by subtracting the prior mean\n",
    "\n",
    "        # Compute alpha for efficient posterior mean computation\n",
    "        alpha = K_obs_inv @ y\n",
    "\n",
    "        self.state = PosteriorState(X_train=X_obs, y_train=y, K_inv=K_obs_inv, alpha=alpha, cholesky_factor=L)\n",
    "\n",
    "    def predict(self, X_query: np.ndarray) -> PredictionResult:\n",
    "        \"\"\"\n",
    "        Predict the mean and variance at new query points.\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "\n",
    "        # Don't worry about this line if it seems confusing, it just computes the prior mean for the query points.\n",
    "        prior_mean = (\n",
    "            self.prior_mean(X_query) if callable(self.prior_mean) else np.full((len(X_query), 1), self.prior_mean)\n",
    "        ).flatten()\n",
    "\n",
    "        if state is None:\n",
    "            K_qq = self.kernel(X_query)  # Prior covariance for the query points.\n",
    "            return PredictionResult(\n",
    "                X_query=X_query,\n",
    "                mean=prior_mean,\n",
    "                variance=np.diag(K_qq),\n",
    "                covariance=K_qq,\n",
    "            )\n",
    "\n",
    "        # Predicting the posterior mean\n",
    "        K_qt = YourCodeHere(\"Compute the covariance between query points and training points.\")  # Your code here.\n",
    "\n",
    "        # K_query: (n_query, n_train) - alpha: (n_train, 1) - prior_mean: (n_query, 1)\n",
    "        mu_posterior = YourCodeHere(\"Compute the posterior mean at the query points, its a dot product of the covariance between query points and training points with the alpha vector.\")  # Your code here.\n",
    "        mu_posterior = mu_posterior.flatten()  # Flatten the mean to make it a 1D array.\n",
    "        mu_posterior += prior_mean  # Add the prior mean to the posterior mean\n",
    "\n",
    "        # Predicting the posterior covariance\n",
    "        K_qq = YourCodeHere(\"Covariance matrix for the query points (n_query, n_query).\")  # Your code here.\n",
    "        cov_posterior = YourCodeHere(\"Compute the posterior covariance matrix - (n_query, n_query).\")  # Your code here.\n",
    "\n",
    "        # Compute the posterior variance as the diagonal of the covariance matrix\n",
    "        var_posterior = np.diag(cov_posterior).copy() # (n_query, )\n",
    "        var_posterior[var_posterior < 0] = 0  # Ensure non-negative variance\n",
    "\n",
    "        return PredictionResult(X_query=X_query, mean=mu_posterior, variance=var_posterior, covariance=cov_posterior)\n",
    "\n",
    "    def sample(self, X_query: np.ndarray, n_samples: int = 1, cov_jitter: float = 1e-6) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sample from the posterior distribution at query points.\n",
    "        \"\"\"\n",
    "        prediction = self.predict(X_query)\n",
    "        samples = np.random.multivariate_normal(\n",
    "            prediction.mean.flatten(),\n",
    "            prediction.covariance + np.eye(prediction.covariance.shape[0]) * cov_jitter,\n",
    "            n_samples,\n",
    "            check_valid=\"ignore\",\n",
    "        )\n",
    "        return samples\n",
    "\n",
    "    def log_marginal_likelihood(self) -> float:\n",
    "        \"\"\"\n",
    "        Compute the log marginal likelihood of the observed data.\n",
    "        \"\"\"\n",
    "        y = self.state.y_train\n",
    "        K_inv = self.state.K_inv\n",
    "        L = self.state.cholesky_factor\n",
    "\n",
    "        lml = -0.5 * y.T @ K_inv @ y - np.sum(np.log(np.diag(L))) - len(y) / 2 * np.log(2 * np.pi)\n",
    "        return lml.flatten()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try using the model, the cell below just checks some output shapes to ensure everything works as\n",
    "intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_obs = np.array([0.3, 0.5, 0.7]).reshape(-1, 1)\n",
    "y_obs = np.array([-0.5, 1, 0]).reshape(-1, 1)\n",
    "X_query = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Construct GP\n",
    "kernel = Constant(1.0) * RadialBasis(length_scale=0.1) + Noise(noise_level=1e-6)\n",
    "gp = GaussianProcess(kernel)\n",
    "\n",
    "# First we can check the prior mean and covariance at the query points\n",
    "prior_prediction = gp.predict(X_query)  # Predict the prior mean and variance at query points\n",
    "\n",
    "print(f\"{prior_prediction.mean.shape = }\")  # Should be (100,)\n",
    "print(f\"{prior_prediction.covariance.shape = }\")  # Should be (100, 100)\n",
    "assert prior_prediction.mean.shape == (100,), \"Prior mean should be of shape (100,)\"\n",
    "assert prior_prediction.covariance.shape == (100, 100), \"Prior covariance should be of shape (100, 100)\"\n",
    "\n",
    "# Then the posterior mean and covariance after conditioning on the observed data\n",
    "gp.condition(X_obs, y_obs)  # Condition the GP with observed data\n",
    "posterior_prediction = gp.predict(X_query)  # Predict the prior mean and variance at query points\n",
    "\n",
    "print(f\"{posterior_prediction.mean.shape = }\")  # Should be (100,)\n",
    "print(f\"{posterior_prediction.covariance.shape = }\")  # Should be (100, 100)\n",
    "assert posterior_prediction.mean.shape == (100,), \"Posterior mean should be of shape (100,)\"\n",
    "assert posterior_prediction.covariance.shape == (100, 100), \"Posterior covariance should be of shape (100, 100)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to try our GP model and plot the results.\n",
    "\n",
    "The `plot_gp_prediction`-function is just a simple little plotting utility function, you can see its definition using a notebook magic `plot_gp_prediction??` in a code cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct GP\n",
    "kernel = Constant(1.0) * RadialBasis(length_scale=0.1) + Noise(noise_level=1e-6)\n",
    "prior_mean = lambda x: 2* np.ones_like(x)  # Constant prior mean function - you can try changing this.\n",
    "gp = GaussianProcess(kernel=kernel, prior_mean=prior_mean)\n",
    "\n",
    "# Make some data\n",
    "X_obs = np.array([0.3, 0.5, 0.7]).reshape(-1, 1)\n",
    "y_obs = np.array([-0.5, 1, 0]).reshape(-1, 1)\n",
    "X_query = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "# Plotting \n",
    "fig, axes = plt.subplots(1, 2, figsize=(2 * 4, 4), sharey=True)\n",
    "for ax, condition in zip(axes, [False, True]):\n",
    "    if condition:\n",
    "        gp.condition(X_obs, y_obs)  # Condition the GP with observed data\n",
    "        ax.scatter(X_obs, y_obs, color=\"red\", label=\"Observations\")\n",
    "\n",
    "        lml = gp.log_marginal_likelihood()\n",
    "        ax.set_title(\"Conditioned GP - LML: {:.2f}\".format(lml))\n",
    "    else:\n",
    "        ax.set_title(\"Unconditioned GP\")\n",
    "\n",
    "    # Query the GP for predictions\n",
    "    prediction = gp.predict(X_query)\n",
    "    plot_gp_prediction(ax, prediction)\n",
    "\n",
    "    samples = gp.sample(X_query, n_samples=10)\n",
    "    for sample in samples:\n",
    "        ax.plot(X_query, sample, color=\"gray\", alpha=0.5, zorder=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Note:</b> If you skipped the previous section you can import and use \n",
    "my version of the `GaussianProcess`-class by uncommenting the import block below.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to import the GaussianProcess class and related kernels\n",
    "# Keep commented if you want to use your own version of the GaussianProcess class.\n",
    "# from bayes_nanospace2025 import GaussianProcess, RadialBasis, Noise, Constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will test our GP implementation on the function defined in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    \"\"\"\n",
    "    A simple function to model.\n",
    "    \"\"\"\n",
    "    return np.sin(2 * np.pi * x) + 0.1 * np.random.randn(*x.shape)\n",
    "\n",
    "# Generate some training data\n",
    "np.random.seed(42)  # For reproducibility\n",
    "n_data = 25\n",
    "X_obs = np.random.uniform(-5, 5, size=(n_data, 1))\n",
    "y_obs = f(X_obs)\n",
    "\n",
    "X_query = np.linspace(-5, 5, 1000).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can setup the `GaussianProcess` model, condition it on the data and plot its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup GP & predict\n",
    "kernel = Constant(1) * RadialBasis(length_scale=0.5) + Noise(noise_level=1e-6)\n",
    "gp = GaussianProcess(kernel=kernel, prior_mean=0)\n",
    "gp.condition(X_obs, y_obs)\n",
    "prediction = gp.predict(X_query)\n",
    "\n",
    "# Plotting \n",
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "plot_gp_prediction(ax, prediction)\n",
    "ax.scatter(X_obs, y_obs, color=\"red\", label=\"Observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our nicer implementation we can compute the marginal loglikelihood landscape by creating a grid over kernel parameters and plot the landscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the lml landscape\n",
    "noise_levels = np.geomspace(0.001, 0.75, 50)\n",
    "length_scales = np.geomspace(0.01, 5.0, 50)\n",
    "lenth_mesh, noise_mesh = np.meshgrid(length_scales, noise_levels)\n",
    "lml_values = np.zeros((len(noise_levels), len(length_scales)))\n",
    "for i, noise in enumerate(noise_levels):\n",
    "    for j, length_scale in enumerate(length_scales):\n",
    "        kernel = Constant(1) * RadialBasis(length_scale=length_scale) + Noise(noise_level=noise)\n",
    "        gp = GaussianProcess(kernel=kernel, prior_mean=0)\n",
    "        gp.condition(X_obs, y_obs)\n",
    "        lml_values[i, j] = gp.log_marginal_likelihood()\n",
    "\n",
    "lml_values = np.clip(lml_values, a_min=-50, a_max=None)  # Clip values for better visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "# Make the contour plot\n",
    "ax.contour(length_scales, noise_levels, lml_values, levels=20, colors=\"black\", linestyles='-')\n",
    "c = ax.contourf(length_scales, noise_levels, lml_values, levels=20, cmap=\"Purples\")\n",
    "\n",
    "# Plot the maximum log marginal likelihood\n",
    "index_0 = np.unravel_index(np.argmax(lml_values), lml_values.shape)\n",
    "index_1 = np.unravel_index(np.argmax(lml_values - 1000 * (length_scales < 1)), lml_values.shape)\n",
    "\n",
    "for max_index in [index_0, index_1]:\n",
    "    ax.plot(length_scales[max_index[1]], noise_levels[max_index[0]], \"ro\", markersize=10, label=\"Max LML\")\n",
    "\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xlabel(\"Length Scale\")\n",
    "ax.set_ylabel(\"Noise Level\")\n",
    "ax.set_title(\"Log Marginal Likelihood Landscape\")\n",
    "fig.colorbar(c, ax=ax, label=\"Log Marginal Likelihood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now make a GP with the optimal hyperparameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for ax, max_index in zip(axes, [index_0, index_1]):\n",
    "    length_scale = length_scales[max_index[1]]\n",
    "    noise_level = noise_levels[max_index[0]]\n",
    "\n",
    "    kernel = Constant(1) * RadialBasis(length_scale=length_scale) + Noise(noise_level=noise_level)\n",
    "    gp = GaussianProcess(kernel=kernel, prior_mean=0)\n",
    "    gp.condition(X_obs, y_obs)\n",
    "    prediction = gp.predict(X_query)\n",
    "\n",
    "    ax.scatter(X_obs, y_obs, color=\"red\", label=\"Observations\")\n",
    "    plot_gp_prediction(ax, prediction)\n",
    "    ax.set_title(f\"Length Scale: {length_scale:.2f}, Noise Level: {noise_level:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Takeaway:</b> The log marginal likelihood landscape proposes two hypothesis for our data; one that is quickly varying with low variance that closely fits the data and one that is slowly varying with high variance. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grid search is not particularly efficient, so normally gradient-based optimization is employed. \n",
    "The analytical gradients of the LML wrt. kernel hyperparameters can be derived but it's tedious - \n",
    "so we will stick to a gradient-free optimization procedure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_hyperparameters(initial_guess, X_obs, y_obs):\n",
    "    from scipy.optimize import minimize\n",
    "\n",
    "    def calculate_lml(length_scale, noise_level, amplitude):\n",
    "        kernel = Constant(amplitude) * RadialBasis(length_scale=length_scale) + Noise(noise_level=noise_level)\n",
    "        gp = GaussianProcess(kernel=kernel, prior_mean=0)\n",
    "        gp.condition(X_obs, y_obs)\n",
    "        return gp.log_marginal_likelihood()\n",
    "    \n",
    "    bounds = [(1e-6, 5.0), (1e-3, 2.0), (0.1, 10)]  # Bounds for length_scale and noise_level\n",
    "    result = minimize(\n",
    "        lambda x: -calculate_lml(x[0], x[1], x[2]),  # We minimize the negative log marginal likelihood\n",
    "        initial_guess,\n",
    "        bounds=bounds,\n",
    "    )\n",
    "    print(f\"Optimal Length Scale: {result.x[0]:.4f}, Optimal Noise Level: {result.x[1]:.4f}, Optimal Amplitude: {result.x[2]:.4f}\")\n",
    "    \n",
    "\n",
    "    return result.x  # Return the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_1 = optimize_hyperparameters([0.1, 0.1, 1.0], X_obs, y_obs) # Initial guess for length_scale, noise_level, and amplitude\n",
    "set_2 = optimize_hyperparameters([2, 0.5, 1.0], X_obs, y_obs) # Another initial guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "for ax, x in zip(axes, [set_1, set_2]):\n",
    "    kernel = Constant(x[2]) * RadialBasis(length_scale=x[0]) + Noise(noise_level=x[1])\n",
    "    gp = GaussianProcess(kernel=kernel, prior_mean=0)\n",
    "    gp.condition(X_obs, y_obs)\n",
    "\n",
    "    predictions = gp.predict(X_query)\n",
    "\n",
    "    plot_gp_prediction(ax, predictions)\n",
    "    ax.scatter(X_obs, y_obs, color=\"red\", label=\"Observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Takeaway:</b> By optimizing the kernel parameters we can also \n",
    "find the two ways of describing the data that we saw before. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now seen how Gaussian Processes can be used for regression.\n",
    "\n",
    "- GP's are distributions over functions. \n",
    "- The type of functions are controlled through the prior mean and coviariance/kernel.\n",
    "- Knowledge about the data we want to model can be encoded through the choice of kernel. \n",
    "- Conditioning on observed data makes the functions in the distribution fit the observations. \n",
    "- The posterior mean is average of all the functions in the distribution after conditioning. \n",
    "- The posterior covariance, variance and standard deviation gives us the ability to evaluate the models confidence about its predictions.\n",
    "- The marginal log likelihood lets us evaluate the model's ability to describe the data, allowing us to pick appropriate hyperparameters.\n",
    "\n",
    "\n",
    "Ideally I've left you with the impression that they are versatile models. \n",
    "\n",
    "I hope you will try to apply a GP to your own research. If you're working with relatively \n",
    "small datasets (<2000ish observations) then GP's are ideal and will provide you a very good baseline model \n",
    "that you can use directly or use as a benchmark for other models. \n",
    "\n",
    "### Resources \n",
    "\n",
    "I've listed a few resources that you might find useful\n",
    "\n",
    "| Resource | Description| \n",
    "| -------- | --------------- |\n",
    "| [Gaussian Processes for Machine Learning](https://gaussianprocess.org/gpml/chapters/RW.pdf) | Defacto best book on GPs for machine learning, covers many topics in detail | \n",
    "| [GPyTorch](https://gpytorch.ai/) | PyTorch implementations of various types of GPR's |\n",
    "| [GPJax](https://docs.jaxgaussianprocesses.com/) | Jax implementations of various types of GPR's |\n",
    "| [Scikit-learn GP](https://scikit-learn.org/stable/modules/gaussian_process.html) |Â Python implementation of GP regression and classification. Good starting point for comparison with more sophisticated implementations. |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
