{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import bayes_nanospace2025\n",
    "    print(\"Already installed\")\n",
    "except ImportError:\n",
    "    %pip install -q \"bayes_nanospace2025 @ git+https://github.com/Mads-PeterVC/nanospace2025.git\" # Install from GitHub. \n",
    "    print(\"Installed bayes_nanospace2025 from GitHub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> <b>Warning:</b> If packages weren't already installed, please refresh this page in your <b> browser</b>, you shouldn't need to restart the Jupyter kernel, as otherwise the widgets used won't show properly. Sorry for the inconvenience! </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bayes_nanospace2025 import GaussianProcess, RadialBasis, Noise, Constant, YourCodeHere\n",
    "from bayes_nanospace2025.tutorial.gp import PredictionResult\n",
    "from bayes_nanospace2025.tutorial.test_functions import himmelblau\n",
    "from bayes_nanospace2025.tutorial import plot_bo, plot_bo_2d\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization with GPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Bayesian optimization (BO) we wish to optimize some, possibly blackbox, function that is \n",
    "typically quite expensive to evaluate such as computationally heavy calculations, e.g. DFT, or laboratory experiments. \n",
    "\n",
    "Formally, we say that we are looking for \n",
    "\n",
    "$$\n",
    "x_{\\mathrm{opt}} = \\mathrm{argmin}_{x \\in D} f(x)\n",
    "$$\n",
    "\n",
    "Wikipedia describes Bayesian optimization like so; \n",
    "> \"Since the objective function is unknown, the Bayesian strategy is to treat it as a random function and **place a prior over it**. The prior captures beliefs about the behavior of the function. After gathering the function evaluations, which are treated as data, the prior is updated to form the **posterior distribution** over the objective function. The posterior distribution, in turn, is used to construct an **acquisition function** that determines the next query point\".\n",
    "\n",
    "We've already seen how GPs describe a distribution over functions with the prior defined by the covariance function (i.e. kernel) and we've seen how to condition the distribution to \n",
    "obtain a posterior distribution where the functions match our observations. The part we haven't seen so far is the use of an **acquisiton function**. A rough outline of the BO algorithm we will develop is as follows;\n",
    "\n",
    "---\n",
    "\n",
    "Initialize\n",
    "\n",
    "- Create a GP model over the domain of interest $D$. \n",
    "\n",
    "And then repeats these steps for a number of iterations\n",
    "\n",
    "1. Evaluate an acquistion function at points in the domain, the acquisition function is based on the GP. \n",
    "2. Choose the next observation according to the acquisition function ($\\mathrm{argmin}$ or $\\mathrm{argmax}$ depending on definition).\n",
    "3. Evaluate the objective function at \"acquired\"-coordinate $x_{\\mathrm{acq}}$ to get $y_{\\mathrm{acq}}$.\n",
    "4. Add the new observation to our set of observations $(X, y)$\n",
    "5. Update the GP with new observations.\n",
    "\n",
    "---\n",
    "\n",
    "An example of an acquisiton function is the lower confidence bound\n",
    "\n",
    "$$\n",
    "\\mathrm{LCB}(x) = \\mu(x) - \\kappa \\sigma(x)\n",
    "$$\n",
    "\n",
    "Where $\\mu(x)$ is the predicted value and $\\sigma(x)$ is the predicted standard deviation 'uncertainty' and $\\kappa$ is a weighting factor between the two terms. \n",
    "This acquisition function will thus have minimas either at locations where the mean is low or where there's a large uncertainty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization for a 1D function.\n",
    "\n",
    "Well start by implementing BO for a 1D function, so we can easily visualize how it progresses. \n",
    "\n",
    "First we'll choose some settings and setup some lists for storing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the GP\n",
    "kernel = Constant(1.0) * RadialBasis(length_scale=1) + Noise(0.001)\n",
    "gp = GaussianProcess(kernel=kernel)\n",
    "\n",
    "objective_function = himmelblau  # Define the objective function\n",
    "\n",
    "# Settings:\n",
    "n_iterations = 25\n",
    "kappa = 2\n",
    "X_query = np.linspace(-5.0, 5.0, 200).reshape(-1, 1)  # Query points for predictions\n",
    "\n",
    "# Lists for observations\n",
    "X_obs = []\n",
    "y_obs = []\n",
    "predictions = []\n",
    "acquisition_functions = []\n",
    "acquisition_indices = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to write our optimization loop. In the cell below you need to fill in the following; \n",
    "\n",
    "1. Making predictions with the GP. \n",
    "2. Calculating the lower-confidence bound acquisition function.\n",
    "3. Determine the index of the next query point to evaluate with the objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(n_iterations):\n",
    "    # Make predictions with the GP\n",
    "    prediction = gp.predict(X_query)  # YCH: Make predictions with the GP, using the predict method.\n",
    "    predictions.append(prediction)\n",
    "\n",
    "    # Construct the LCB acquisition function\n",
    "    # Remember that the 'prediction' object contains 'prediction.mean' and 'prediction.variance' -> std = sqrt(variance)\n",
    "    lcb = prediction.mean - kappa * np.sqrt(prediction.variance) # YCH: Implement the LCB\n",
    "    acquisition_functions.append(lcb)\n",
    "\n",
    "    # Find the next point to sample (the minimum of the LCB)\n",
    "    # Use np.argmin to find the index of the minimum value in lcb\n",
    "    if len(X_obs) > 0:\n",
    "        index = np.argmin(lcb) # YCH: Determine the index corresponding to the lowest LCB value.\n",
    "    else: # If no observations pick a random index.\n",
    "        index = np.random.randint(low=0, high=X_query.shape[0])\n",
    "\n",
    "    next_x = X_query[index]\n",
    "\n",
    "    # Evaluate the function at the next point\n",
    "    next_y = objective_function(next_x.reshape(1, -1))\n",
    "\n",
    "    # Add new observation to the lists\n",
    "    X_obs.append(next_x)\n",
    "    y_obs.append(next_y)\n",
    "    acquisition_indices.append(index)\n",
    "\n",
    "    # Update the GP with the new observations\n",
    "    gp.set_prior_mean(np.mean(y_obs)) # Sets the prior mean of the GP to the mean of the observed values.\n",
    "\n",
    "    X_obs_arr = np.array(X_obs).reshape(-1, 1)\n",
    "    y_obs_arr = np.array(y_obs).reshape(-1, 1)\n",
    "    gp.condition(X_obs_arr, y_obs_arr)  # Update the GP with the new observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_plot = X_query.flatten()\n",
    "truth = himmelblau(X_query).flatten()\n",
    "plot_bo(predictions=predictions, X_obs=X_obs, y_obs=y_obs, \n",
    "                      acquisition_values=acquisition_functions, acquisition_indices=acquisition_indices, \n",
    "                      X_plot=X_plot, truth=truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimizer class\n",
    "\n",
    "Now we've seen the concept of Bayesian Optimization with GPs in action we can wrap it up \n",
    "in a little class to avoid having to repeat code over and over. \n",
    "\n",
    "To get some flexibility we will first define an `AcquisitionFunction`-class that, unsurprisingly\n",
    "defines the acquisition function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcquisitionFunction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute(self, prediction: PredictionResult, gp: GaussianProcess, X_query: np.ndarray) -> tuple[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Returns the index of the next point to sample and the acquisition function values over the query points.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"This method should be implemented by subclasses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then implement the lower-confidence bound in this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowerConfidenceBound(AcquisitionFunction):\n",
    "    def __init__(self, kappa: float = 2.0):\n",
    "        super().__init__()\n",
    "        self.kappa = kappa\n",
    "\n",
    "    def compute(self, prediction: PredictionResult, gp: GaussianProcess, X_query: np.ndarray) -> tuple[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute the Lower Confidence Bound (LCB) acquisition function.\n",
    "        \"\"\"\n",
    "        lcb = prediction.mean - self.kappa * np.sqrt(prediction.variance)  # YCH: Construct the LCB acquisition function\n",
    "\n",
    "        if np.var(lcb) == 0: # If the variance is zero all values are the same, so we pick a random index.\n",
    "            next_index = np.random.randint(low=0, high=X_query.shape[0])\n",
    "        else:\n",
    "            next_index = np.argmin(lcb)  # YCH: Find the index of the minimum value in lcb\n",
    "        return next_index, lcb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like with our GP I find it useful to have a data class to carry our results, I've \n",
    "created a minimal one below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "\n",
    "@dataclass\n",
    "class OptimizationResult:\n",
    "    X_obs: list = field(default_factory=list)  # List of observed input points\n",
    "    y_obs: list = field(default_factory=list)  # List of observed output values\n",
    "    predictions: list[np.ndarray] = field(default_factory=list)  # List of predictions made by the GP\n",
    "    acquisition_values: list[np.ndarray] = field(default_factory=list)  # List of acquisition function values\n",
    "    acquisition_indices: list[int] = field(default_factory=list) # List of acquisition indices\n",
    "\n",
    "    def update(\n",
    "        self, X_new: np.ndarray, y_new: np.ndarray, prediction: np.ndarray, acq_values: np.ndarray, acq_index: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Update the optimization result with new observations.\n",
    "        \"\"\"\n",
    "        self.X_obs.append(X_new)\n",
    "        self.y_obs.append(y_new)\n",
    "        self.predictions.append(prediction)\n",
    "        self.acquisition_values.append(acq_values)\n",
    "        self.acquisition_indices.append(acq_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can wrap things up - like always there are many ways of going about it. \n",
    "\n",
    "I've laid out one option below with the following methods;\n",
    "\n",
    "- `iterate`: Run an iteration of the BO, making GP predictions and evaluating the acquisition function.\n",
    "- `query_objective`: Query the objective function.\n",
    "- `update_gp`: Update the GP with new data.\n",
    "- `optimize`: Main method.\n",
    "\n",
    "You will need to fill in the code for these things; \n",
    "\n",
    "1. Making predictions with the GP in the `iterate`-method.\n",
    "2. Using the acquisition function instance to (`self.acquisition_function`) to compute the acquisition function, again in `iterate`\n",
    "3. Fill in the code in the `optimize`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianOptimizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        objective_function: Callable,\n",
    "        gp: GaussianProcess,\n",
    "        acquisition_function: AcquisitionFunction,\n",
    "        objective_input_index: bool = False,\n",
    "    ):\n",
    "        self.objective_function = objective_function\n",
    "        self.gp = gp\n",
    "        self.acquisition_function = acquisition_function\n",
    "        self.objective_input_index = objective_input_index\n",
    "\n",
    "    def iterate(self, X_query):\n",
    "        # Make predictions with the GP\n",
    "        prediction = self.gp.predict(X_query)  # YCH: Make predictions with the GP\n",
    "\n",
    "        # Compute the acquisition function\n",
    "        acq_index, acq_values = self.acquisition_function.compute(prediction, self.gp, X_query)  # YCH: Compute the acquisition function, by calling the method on the acquisition function instance\n",
    "\n",
    "        # Find the next point to sample\n",
    "        next_x, next_y = self.query_objective(X_query, acq_index)\n",
    "\n",
    "        return next_x, next_y, prediction, acq_values, acq_index\n",
    "\n",
    "    def query_objective(self, X_query, acq_index):\n",
    "        next_x = X_query[acq_index]\n",
    "        if not self.objective_input_index:\n",
    "            next_y = self.objective_function(next_x.reshape(1, -1))\n",
    "        else: # If the objective function expects an index, we pass the index directly.\n",
    "            # This is just to make life easier later, don't worry about it.\n",
    "            next_y = self.objective_function(acq_index)\n",
    "        return next_x, next_y\n",
    "\n",
    "    def update_gp(self, results: OptimizationResult):\n",
    "        \"\"\"\n",
    "        Update the Gaussian Process with new observations.\n",
    "        \"\"\"\n",
    "        y_obs_arr = np.array(results.y_obs).reshape(-1, 1)  # Ensure y_obs is a 2D array\n",
    "        X_obs_arr = np.atleast_2d(np.array(results.X_obs))\n",
    "\n",
    "        self.gp.set_prior_mean(np.mean(y_obs_arr))\n",
    "        self.gp.condition(X_obs_arr, y_obs_arr)\n",
    "\n",
    "    def optimize(self, X_query: np.ndarray, n_iterations: int):\n",
    "        results = OptimizationResult()\n",
    "\n",
    "        for _ in range(n_iterations):\n",
    "            # Call the iterate method.\n",
    "            next_x, next_y, prediction, acq_values, acq_index = self.iterate(X_query)  # YCH: Run an iteration of the BO\n",
    "\n",
    "            # Update our results with new observations\n",
    "            results.update(next_x, next_y, prediction, acq_values, acq_index)\n",
    "\n",
    "            # Update the GP with new data\n",
    "            self.update_gp(results)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try our class on the Himmelblau function and confirm that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimizer(\n",
    "    objective_function=himmelblau,\n",
    "    gp=GaussianProcess(kernel=Constant(1.0) * RadialBasis(length_scale=1) + Noise(0.001)),\n",
    "    acquisition_function=LowerConfidenceBound(kappa=2.0),\n",
    ")\n",
    "\n",
    "X_query = np.linspace(-5.0, 5.0, 100).reshape(-1, 1)  # Query points for predictions\n",
    "result = optimizer.optimize(X_query, n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = himmelblau(X_query).flatten()\n",
    "plot_bo(result, X_plot=X_query.flatten(), truth=truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thompson sampling acquisition function\n",
    "\n",
    "We can now try changing our acquisition function, a particularly interesting one is Thompson sampling, \n",
    "where rather than calculating some function of the mean and variance we simply draw a sample from the GP. \n",
    "The next query point selected to evaluate with the objective function is then the minimum of that sample. \n",
    "\n",
    "So to implement Thompson sampling just two steps are required: \n",
    "\n",
    "1. Draw a sample from the GP-model (`gp.sample`)\n",
    "2. Find the index of the minimum of this sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling(AcquisitionFunction):\n",
    "    \n",
    "    def compute(self, prediction: PredictionResult, gp: GaussianProcess, X_query: np.ndarray) -> tuple[int, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Compute the Thompson sampling acquisition function.\n",
    "        \"\"\"\n",
    "        sample = gp.sample(X_query, n_samples=1).T  # YCH: Sample from the GP with the .sample method.\n",
    "        next_index = np.argmin(sample) # YCH: Find the index of the minimum value in the sampled function\n",
    "        return next_index, sample.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimizer(\n",
    "    objective_function=himmelblau,\n",
    "    gp=GaussianProcess(kernel=Constant(1.0) * RadialBasis(length_scale=1) + Noise(0.001)),\n",
    "    acquisition_function=ThompsonSampling()\n",
    ")\n",
    "\n",
    "X_query = np.linspace(-5.0, 5.0, 100).reshape(-1, 1)  # Query points for predictions\n",
    "result = optimizer.optimize(X_query, n_iterations=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = himmelblau(X_query).flatten()\n",
    "plot_bo(result, X_plot=X_query.flatten(), truth=truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2D Bayesian Optimization\n",
    "\n",
    "We can also try it for a 2D problem\n",
    "\n",
    "Play around with the settings below and compare with Wikipedia's article on [Himmelblau's function](https://en.wikipedia.org/wiki/Himmelblau%27s_function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 25)\n",
    "X1, X2 = np.meshgrid(x, x)\n",
    "X_query = np.vstack([X1.ravel(), X2.ravel()]).T\n",
    "\n",
    "acquisition_function = ThompsonSampling()  # YCH: Define the acquisition function. Try both LCB & ThompsonSampling\n",
    "gp = GaussianProcess(kernel=Constant(1.0) * RadialBasis(length_scale=1) + Noise(0.001))\n",
    "\n",
    "optimizer = BayesianOptimizer(objective_function=himmelblau, gp=gp, acquisition_function=acquisition_function)\n",
    "result = optimizer.optimize(X_query, n_iterations=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_bo_2d(result, X1=X1, X2=X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization for Atomistic Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate Bayesian optimization for atomistic systems we will investigate \n",
    "the energy landscape of a small metal cluster. The cell below uses the `ngl`-widget \n",
    "of ASE to display the cluster, you can use the slider to see the configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_nanospace2025.tutorial.atoms_dataset import get_atoms_data\n",
    "from ase.visualize import view\n",
    "\n",
    "atoms_data = get_atoms_data()\n",
    "\n",
    "view(atoms_data, viewer=\"ngl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is the potential energy, we will use an effective-medium-theory (EMT) calculator, \n",
    "but this could be replaced with a more sophisticated potential, such as DFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.calculators.emt import EMT\n",
    "\n",
    "def objective_function(index):\n",
    "    if isinstance(index, np.ndarray):\n",
    "        index = index.flatten()[0]\n",
    "\n",
    "    atoms = atoms_data[index]\n",
    "    atoms.calc = EMT()  # Set the calculator\n",
    "    energy = atoms.get_potential_energy()  # Calculate the potential energy\n",
    "    return energy  # Return the energy as the objective function value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_function(np.array([[0]]))  # Example usage of the objective function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_query = np.arange(len(atoms_data)).reshape(-1, 1)  # Query points for predictions\n",
    "acquisition_function = LowerConfidenceBound(kappa=2.0)  # Define the acquisition function\n",
    "gp = GaussianProcess(kernel=Constant(1.0) * RadialBasis(length_scale=10) + Noise(0.0001), prior_mean=10)\n",
    "\n",
    "optimizer = BayesianOptimizer(objective_function=objective_function, gp=gp, acquisition_function=acquisition_function)\n",
    "\n",
    "result = optimizer.optimize(X_query, n_iterations=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.array([objective_function(i) for i in range(len(atoms_data))])  # Calculate the true values for the atoms\n",
    "X_plot = np.arange(len(atoms_data))  # X values for plotting\n",
    "\n",
    "plot_bo(result, X_plot=X_plot, truth=truth)  # Call the function to plot the Bayesian optimization results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Takeaway:</b> The GP posterior mean doesn't generalize, notice for example that an observation at around index 40 does not influence the prediction at index 80 or vice versa - even though these are symmetric configurations. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptors for atomistic systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This is rather unimpressive! A large part of the reason is that the coordinates, our features, we use are not \n",
    "particularly useful for atomic systems, recall that we want something that has these invariances: \n",
    "\n",
    "- **Permutational**: Swapping of atoms of the same species should not change the feature.\n",
    "- **Translational**: A rigid translation should not change the feature. \n",
    "- **Rotational**: A rotation should not change the feature. \n",
    "\n",
    "The features we use are just the index of the atoms in the list, and they have none of these properties. \n",
    "A relatively simple descriptor that includes all of these properties is the eigenvalue spectrum of the Coulomb matrix, from this, now kind of old, paper [Fast and Accurate Modeling of Molecular Atomization Energies with Machine Learning](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.108.058301). \n",
    "Here the Coulomb-matrix is defined as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "C_{ij} =\n",
    "\\begin{cases}\n",
    "    & \\frac{Z_i Z_j}{r_{ij}}, \\quad i \\neq j \\\\\n",
    "    & 0.5 Z_i^2, \\quad i = j\n",
    "\\end{cases}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The matrix itself has rotational and translational invariance and by using the sorted eigenvalue spectrum permutational \n",
    "invariance is obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coulomb_matrix(atoms, return_matrix=False):\n",
    "    d_mat = atoms.get_all_distances() + np.eye(len(atoms)) * 10  # n_atoms x n_atoms matrix.\n",
    "    species = atoms.get_atomic_numbers().reshape(-1, 1)  # Get atomic numbers of the atoms\n",
    "    species_mat = species @ species.T  # Outer product to create a matrix of atomic numbers\n",
    "\n",
    "    c_matrix = species_mat / d_mat  # YCH: Compute the Coulomb matrix using species_mat and d_mat.\n",
    "    c_matrix[np.diag_indices_from(c_matrix)] = 0.5 * species.flatten() ** 2\n",
    "\n",
    "    eigenvalues = np.linalg.eigvalsh(c_matrix)  # Calculate the eigenvalues of the Coulomb matrix\n",
    "    if return_matrix:\n",
    "        return eigenvalues, c_matrix\n",
    "    return eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that this works as we would like by plotting it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.visualize.plot import plot_atoms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.build import molecule\n",
    "\n",
    "def get_benzene_molecule(rotation=None, translation=None, permutation=None, rattle=None):\n",
    "    atoms = molecule('C6H6')\n",
    "    atoms.cell = np.eye(3) * 10\n",
    "    atoms.center()\n",
    "\n",
    "    if rotation is not None:\n",
    "        atoms.rotate(rotation, v=(0, 0, 1), center='COM', rotate_cell=False)\n",
    "    if translation is not None:\n",
    "        atoms.translate(translation)\n",
    "    if permutation is not None:\n",
    "        atoms = atoms[permutation]\n",
    "    if rattle is not None:\n",
    "        atoms.rattle(rattle, seed=42)\n",
    "\n",
    "    return atoms\n",
    "\n",
    "def plot_atoms_descriptor(axes, atoms):\n",
    "\n",
    "    # PLot the atoms in the first axes\n",
    "    plot_atoms(atoms, ax=axes[0])\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Calculate the Coulomb matrix and its eigenvalues\n",
    "    eigenvalues, c_matrix = calculate_coulomb_matrix(atoms, return_matrix=True)\n",
    "\n",
    "    base_line = get_benzene_molecule()\n",
    "    eigenvalues_base, _ = calculate_coulomb_matrix(base_line, return_matrix=True)\n",
    "\n",
    "    # Plot the Coulomb matrix in the second axes\n",
    "    im = axes[1].imshow(c_matrix, cmap='viridis', interpolation='nearest')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Plot the eigenvalues in the third axes\n",
    "    axes[2].bar(range(len(eigenvalues)), eigenvalues, facecolor='mediumpurple', edgecolor='black')\n",
    "\n",
    "    mse = np.mean((eigenvalues - eigenvalues_base) ** 2)\n",
    "    axes[2].text(0.5, 0.95, f'MSE: {mse:.2f}', transform=axes[2].transAxes, ha='center', va='top')\n",
    "\n",
    "    #axes[2].set_title('Eigenvalues of Coulomb Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertubations = {\n",
    "    'original': {},\n",
    "    'rotated': {'rotation': 30},\n",
    "    'translated': {'translation': [1.5, 1.5, 0]},\n",
    "    'permutated': {\n",
    "        'permutation': np.random.permutation(np.arange(12))\n",
    "    },\n",
    "    'rattled': {'rattle': 0.5}\n",
    "}\n",
    "\n",
    "sz = 2\n",
    "n_tests = len(pertubations)\n",
    "fig, axes = plt.subplots(3, n_tests, figsize=(n_tests*sz, 3*sz), layout='constrained')\n",
    "for i, (key, params) in enumerate(pertubations.items()):\n",
    "    atoms = get_benzene_molecule(**params)\n",
    "    plot_atoms_descriptor(axes[:, i], atoms)\n",
    "    axes[0, i].set_title(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Takeaway:</b> The eigenvalue spectrum of the Coulomb \n",
    "matrix is invariant to rotation, translation, permutation as seen in the first four columns. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the task of choosing hyperparameters easier we will also scale each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-max scaling of the features\n",
    "def min_max_scale(X, feature_range=(-1, 1)):\n",
    "    X_min = X.min(axis=0)\n",
    "    X_max = X.max(axis=0)\n",
    "    X_scaled = (X - X_min) / (X_max - X_min)\n",
    "    return X_scaled * (feature_range[1] - feature_range[0]) + feature_range[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization with atomistic descriptors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can set up the Bayesian optimization with Coulomb matrix features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Massage the query points a little bit\n",
    "X_query = np.array([calculate_coulomb_matrix(atoms) for atoms in atoms_data])  # Query points for predictions\n",
    "X_query = min_max_scale(X_query, feature_range=(-1, 1))\n",
    "\n",
    "# Setup the Bayesian optimization\n",
    "acquisition_function = LowerConfidenceBound(kappa=2.0)  # Define the acquisition function\n",
    "kernel = Constant(0.1) * RadialBasis(length_scale=2.0) + Noise(1e-8)\n",
    "gp = GaussianProcess(kernel=kernel, prior_mean=10.0)\n",
    "optimizer = BayesianOptimizer(\n",
    "    objective_function=objective_function,\n",
    "    gp=gp,\n",
    "    acquisition_function=acquisition_function,\n",
    "    objective_input_index=True,  # Set to True to use the index as input for the objective function\n",
    ")\n",
    "\n",
    "# Run the optimization\n",
    "result = optimizer.optimize(X_query, n_iterations=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = np.array([objective_function(i) for i in range(len(atoms_data))])  # Calculate the true values for the atoms\n",
    "X_plot = np.arange(len(atoms_data))  # X values for plotting\n",
    "\n",
    "plot_bo(result, X_plot=X_plot, truth=truth)  # Call the function to plot the Bayesian optimization results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> <b>Takeaway:</b> Now observations add non-trivial correlations, \n",
    "observations have influence across all configurations. For example, an observation at index 40 also determines \n",
    "the prediction at index 80, again due to symmetry that is now captured by the descriptor. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Exercises. \n",
    "\n",
    "Here I will list a few extra exercises, that are more free form, they are not listed in a particular order - so you can start anywhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Implement log marginal likelihood optimzation. \n",
    "\n",
    "Our BO algorithm has static kernel parameters - we'd get better results if we \n",
    "optimized the kernel parameters using the log marginal likelihood between each iteration.\n",
    "\n",
    "You can use the hyperparameter optimization code from the previous tutorial as a starting point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Implement other acquisition functions. \n",
    "\n",
    "There are two closely related acquisition functions called \"Probability of Improvement\" and \"Expected Improvement\" \n",
    "that are probably often better than the LCB/Thompson we've used so far. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Use a different descriptor\n",
    "\n",
    "For the atomic system there are other, likely better, descriptors. A number of them are \n",
    "implemented in the `dscribe`-package - you can try using the Valle-Oganov fingerprint \n",
    "from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Algorithm performance. \n",
    "\n",
    "We've seen Bayesian optimization in action, but we haven't tried to measure how well it works. \n",
    "\n",
    "One metric that is important is the average best objective function value found as a function of the \n",
    "number of iterations. \n",
    "\n",
    "To have a base-line to compare to you can implement a random search where in each iteration a random query-point is picked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Apply BO to your research. \n",
    "\n",
    "For example, if you have a model of some phenomenon that has some parameters you can use BO to \n",
    "find good values for those parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bayes-nanospace2025 (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
